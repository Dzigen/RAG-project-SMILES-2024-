{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/dzigen/Desktop/ITMO/smiles2024/RAG-project-SMILES-2024-\")\n",
    "\n",
    "import pandas as pd \n",
    "import ast\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "from src.Retriever import ThresholdRetriever, ThresholdRetrieverConfig\n",
    "from src.Reader import LLM_model, LLM_Hardw_Conf, LLM_Hyper_Conf\n",
    "from src.utils import RetrieverMetrics, ReaderMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_LOGFILE = './logs/trial_1.json'\n",
    "SAVE_ANSWERS = './logs/answers_1.json'\n",
    "BENCHES_SIZE = 1000\n",
    "\n",
    "# Retriever part\n",
    "BENCHMARKS_INFO = {'mtssquad': {'db': 'v2', 'table': 'v1'}}\n",
    "RETRIEVER_CUSTOM_ARGS = {\n",
    "    \"model_path\": \"/home/dzigen/Desktop/nlp_models/intfloat/multilingual-e5-small\",\n",
    "    \"densedb_kwargs\": {'metadata': {\"hnsw:space\": \"ip\"}},\n",
    "    \"model_kwargs\": {'device':'cuda'},\n",
    "    \"encode_kwargs\": {'normalize_embeddings': True, 'prompt': 'query: '},\n",
    "    \"params\": {'fetch_k': 50, 'threshold': 0.2, 'max_k': 3}\n",
    "}\n",
    "\n",
    "# Reader part\n",
    "READER_CUSTOME_ARGS = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 40,\n",
    "    \"top_p\": 0.95,\n",
    "    \"min_p\": 0.05,\n",
    "    \"typical_p\": 1,\n",
    "    \"max_tokens\": -1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure Retriever-part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "banchmark_paths = {}\n",
    "for name, version in BENCHMARKS_INFO.items():\n",
    "    banchmark_paths[name] = {\n",
    "        'table': f\"../../../data/{name}/tables/{version['table']}/benchmark.csv\",\n",
    "        'dense_db': f\"../../../data/{name}/dbs/{version['db']}/densedb\"\n",
    "    }\n",
    "\n",
    "benchmark_config = {}\n",
    "for name, paths in banchmark_paths.items():\n",
    "    \n",
    "    RETRIEVER_CUSTOM_ARGS['densedb_path'] = banchmark_paths[name]['dense_db']\n",
    "    RETRIEVER_CUSTOM_ARGS['densedb_kwargs']['name'] = name\n",
    "\n",
    "    config = ThresholdRetrieverConfig(**RETRIEVER_CUSTOM_ARGS)\n",
    "\n",
    "    benchmark_config[name] = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузить benchmark-датасет\n",
    "benchmarks_df = {}\n",
    "for name, bench_path in banchmark_paths.items():\n",
    "    benchmarks_df[name] = pd.read_csv(banchmark_paths[name]['table'], sep=';').iloc[:BENCHES_SIZE,:]\n",
    "    benchmarks_df[name]['chunk_ids'] = benchmarks_df[name]['chunk_ids'].map(lambda v: ast.literal_eval(v)) \n",
    "    benchmarks_df[name]['contexts'] = benchmarks_df[name]['contexts'].map(lambda v: ast.literal_eval(v)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/dzigen/Desktop/nlp_models/intfloat/multilingual-e5-small. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# инициализировать ретриверов\n",
    "retrievers = {name: ThresholdRetriever(b_config) for name, b_config in benchmark_config.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure Reader-part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = LLM_Hardw_Conf()\n",
    "config2 = LLM_Hyper_Conf(**READER_CUSTOME_ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Meteor...\n",
      "Loading ExactMatch\n"
     ]
    }
   ],
   "source": [
    "reader_metrics = ReaderMetrics(base_dir=\"/home/dzigen/Desktop/ITMO/smiles2024/RAG-project-SMILES-2024-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = LLM_model(config1, config2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks_score = {}\n",
    "gen_answers = {}\n",
    "for i, name in enumerate(benchmarks_df.keys()):\n",
    "    scores = {\n",
    "        'bleu2': [],\n",
    "        'bleu1': [],\n",
    "        'exact_match': [],\n",
    "        'meteor': []\n",
    "    }\n",
    "    gen_answers[name] = []\n",
    "\n",
    "    bench_size = benchmarks_df[name].shape[0]\n",
    "    for j in range(bench_size):\n",
    "        question = benchmarks_df[name]['question'][j]\n",
    "        relevant_docs = retrievers[name].invoke(question)\n",
    "\n",
    "        raw_gen_answer = reader.generate(question, list(map(lambda v: v[1], relevant_docs)))\n",
    "        gen_answer = raw_gen_answer['choices'][0]['message']['content']\n",
    "        gold_answer = benchmarks_df[name]['answer'][j]\n",
    "\n",
    "        #\n",
    "        gen_answers[name].append(gen_answer)\n",
    "\n",
    "        #\n",
    "        scores['bleu1'].append(reader_metrics.bleu1([gen_answer], [gold_answer]))\n",
    "        scores['bleu2'].append(reader_metrics.bleu2([gen_answer], [gold_answer]))\n",
    "        scores['exact_match'].append(reader_metrics.exact_match([gen_answer], [gold_answer]))\n",
    "        scores['meteor'].append(reader_metrics.meteor([gen_answer], [gold_answer]))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f'''[{j} / {bench_size}] Median scores ({name}):\n",
    "              bleu1: {np.median(scores['bleu1'])}\n",
    "              bleu2: {np.median(scores['bleu2'])}\n",
    "              exact_match: {np.median(scores['exact_match'])}\n",
    "              meteor: {np.median(scores['meteor'])}\n",
    "              \n",
    "              Last sample:\n",
    "              - question: {question}\n",
    "              - gold_answer: {gold_answer}\n",
    "              - gen_answer: {gen_answer}''')\n",
    "\n",
    "    benchmarks_score[name] = scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saveing logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить результат\n",
    "if os.path.exists(SAVE_LOGFILE) or os.path.exists(SAVE_ANSWERS):\n",
    "    print(\"Файл существует!\")\n",
    "    raise ValueError\n",
    "\n",
    "log_data = {'info': BENCHMARKS_INFO, \n",
    "            'benchmark_sizes': BENCHES_SIZE,\n",
    "            'reader': READER_CUSTOME_ARGS,\n",
    "            'retriever': RETRIEVER_CUSTOM_ARGS,\n",
    "            'scores': benchmarks_score}\n",
    "\n",
    "with open(SAVE_LOGFILE, 'w', encoding='utf-8') as fd:\n",
    "    fd.write(json.dumps(log_data, indent=1))\n",
    "\n",
    "with open(SAVE_ANSWERS, 'w', encoding='utf-8') as fd:\n",
    "    fd.write(json.dumps(gen_answers, indent=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
