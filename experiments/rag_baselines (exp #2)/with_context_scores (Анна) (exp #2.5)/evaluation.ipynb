{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "BASE_DIR = \"/home/dzigen/Desktop/Projects/rag_project\"\n",
    "#BASE_DIR = \"/trinity/home/team06/workspace/mikhail_workspace/rag_project\"\n",
    "sys.path.insert(0, BASE_DIR)\n",
    "\n",
    "import pandas as pd \n",
    "import ast\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "from src.Retriever import ThresholdRetriever\n",
    "from src.Scorer import SimilarityScorerConfig\n",
    "from src.Reader import LLM_Model\n",
    "from src.utils import ReaderMetrics, RetrieverMetrics, save_reader_trial_log, prepare_thresholdretriever_configs, prepare_reader_configs, load_benchmarks_df, save_retriever_trial_log\n",
    "from src.utils import evaluate_retriever, evaluate_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 1. Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL = 2\n",
    "SAVE_LOGDIR = f'./logs/stage1/trial{TRIAL}'\n",
    "SAVE_RETRIEVER_HYPERPARAMS = f'{SAVE_LOGDIR}/retriever_hyperparams.json'\n",
    "SAVE_RETRIEVERCACHE = f'{SAVE_LOGDIR}/retriever_cache.json'\n",
    "ADDITIONAL_PARAMS = {\n",
    "    'topk_score_list': 10\n",
    "}\n",
    "\n",
    "BENCHMARKS_MAXSIZE = -1\n",
    "BENCHMARKS_INFO = {'mtssquad': {'db': 'v3', 'table': 'v3'}}\n",
    "\n",
    "# Retriever part\n",
    "RETRIEVER_PARAMS = {\n",
    "    \"model_path\": f\"/home/dzigen/Desktop/PersonalAI/Personal-AI/models/intfloat/multilingual-e5-small\",\n",
    "    \"densedb_kwargs\": {'metadata': {\"hnsw:space\": \"ip\"}},\n",
    "    \"model_kwargs\": {'device':'cuda'},\n",
    "    \"encode_kwargs\": {'normalize_embeddings': True, 'prompt': 'query: '},\n",
    "    \"params\": {'fetch_k': 10, 'threshold': -1, 'max_k': 10}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievers_config, benchmarks_path = prepare_thresholdretriever_configs(BASE_DIR, BENCHMARKS_INFO, RETRIEVER_PARAMS)\n",
    "retriever_metrics = RetrieverMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks_df = load_benchmarks_df(benchmarks_path, BENCHMARKS_MAXSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/dzigen/Desktop/PersonalAI/Personal-AI/models/intfloat/multilingual-e5-small. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "RETRIEVERS = {name: ThresholdRetriever(config) for name, config in retrievers_config.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mtssquad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49908 [00:00<?, ?it/s]/home/dzigen/Desktop/Projects/rag_project/src/utils/evaluation_metrics.py:43: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return (2 * self.precision(pred_cands, gold_cands, k) * self.recall(pred_cands, gold_cands, k)) / (self.precision(pred_cands, gold_cands, k) + self.recall(pred_cands, gold_cands, k))\n",
      "/home/dzigen/miniconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/dzigen/miniconda3/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 49908/49908 [11:17<00:00, 73.68it/s, MRR=0.725, mAP=0.725, Recall=0.822, Precision=0.0822, F1=0.149, NoRelContextScore=nan]\n"
     ]
    }
   ],
   "source": [
    "retriever_scores, retriever_cache_ids, predicted_chunks, cache_relevant_flags = evaluate_retriever(\n",
    "    benchmarks_df, RETRIEVERS, retriever_metrics, show_step=5, topk_score_list=ADDITIONAL_PARAMS['topk_score_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_retriever_trial_log(SAVE_LOGDIR, retriever_scores, SAVE_RETRIEVER_HYPERPARAMS, SAVE_RETRIEVERCACHE, \n",
    "                         predicted_chunks, BENCHMARKS_INFO, BENCHMARKS_MAXSIZE, RETRIEVER_PARAMS, ADDITIONAL_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 2. Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL = 3\n",
    "SAVE_LOGDIR = f'./logs/stage2/trial{TRIAL}'\n",
    "SAVE_READER_HYPERPARAMS = f'{SAVE_LOGDIR}/reader_hyperparams.json'\n",
    "SAVE_READERCACHE = f'{SAVE_LOGDIR}/reader_cache.json'\n",
    "BENCHMARKS_MAXSIZE = 4000\n",
    "\n",
    "ADDITIONAL_PARAMS = {\n",
    "    'score': 'total_n_distance',\n",
    "    'max_k': 10,\n",
    "    'bertscore_model_path': \"ru_electra_medium\",\n",
    "    'load_retriever_cache': f'./logs/stage1/trial{TRIAL}/retriever_cache.json'\n",
    "}\n",
    "\n",
    "BENCHMARKS_INFO = {'ragminibioasq': {'db': 'v1', 'table': 'v1'}}\n",
    "\n",
    "# 'total_n_distance' | 'mean_dependency_distance'\n",
    "# 'Отвечай на вопросы, используя информацию из текстов в списке ниже. Каждому тексту в начале в квадратных скобках поставлена в соответствие вещественная оценка его семантической близости к вопросу: в диапозоне от 0.0 (высокая близость) до 1.0 (низкая близость). Используй эту информацию. Выбирай тексты c достаточно высокими оценками близости для генерации ответа на их основе. Если на основании указанных оценок близости ты не уверена в релевантности данных текстов по отношению к заданному вопросу, то сгенерируй следующий ответ: \"У меня нет ответа на ваш вопрос.\".'\n",
    "\n",
    "READER_PARAMS = {\n",
    "    'prompts': {\n",
    "        \"assistant\": 'Отвечай на вопросы, используя информацию из текстов в списке ниже. Каждому тексту в начале в квадратных скобках поставлена в соответствие вещественная оценка его синтаксической сложности: в диапазоне от 0.0 (низкая сложность) до 1.0 (высокая сложность). Учитывай эти данные. Тексты с более низкой оценкой содержат меньше дополнительной информации и из них легче извлечь релевантную информацию, если такая есть. Если оценка кажется тебе слишком высокой, а другие тексты нерелевантны запросу, то сгенерируй следующий ответ: \"У меня нет ответа на ваш вопрос.\".',\n",
    "        \"system\": \"Ты вопросно-ответная система. Все ответы генерируй на русском языке. По вопросам отвечай кратко, чётко и конкретно. Не генерируй излишнюю информацию.\",\n",
    "    },\n",
    "    'gen': {'max_new_tokens': 512, 'eos_token_id': 79097},\n",
    "    'data_operate': {'batch_size': 1},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, benchmarks_path = prepare_thresholdretriever_configs(BASE_DIR, BENCHMARKS_INFO, RETRIEVER_PARAMS)\n",
    "benchmarks_df = load_benchmarks_df(benchmarks_path, BENCHMARKS_MAXSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_config = prepare_reader_configs(READER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [03:38<00:00, 54.70s/it]\n"
     ]
    }
   ],
   "source": [
    "READER = LLM_Model(reader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Meteor...\n",
      "Loading ExactMatch\n"
     ]
    }
   ],
   "source": [
    "sim_score_config = SimilarityScorerConfig()\n",
    "reader_metrics = ReaderMetrics(BASE_DIR, ADDITIONAL_PARAMS['bertscore_model_path'], sim_score_config, READER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49908/49908 [00:00<00:00, 58561.81it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(ADDITIONAL_PARAMS['load_retrievercache'], 'r', encoding='utf-8') as fd:\n",
    "    predicted_chunks = json.loads(fd.read())\n",
    "\n",
    "contexts = {}\n",
    "for name, chunks in predicted_chunks.items():\n",
    "    contexts[name] = []\n",
    "    for docs in tqdm(chunks):\n",
    "        formated_items = []\n",
    "        for i, doc in enumerate(docs[:ADDITIONAL_PARAMS['max_k']]):\n",
    "            cur_score = doc[0] if ADDITIONAL_PARAMS['score'] == 'cosine' else doc[2][ADDITIONAL_PARAMS['score']]\n",
    "            formated_items.append(f\"{i+1}. [{round(cur_score,5)}] {doc[1]}\")\n",
    "        contexts[name].append(reader_config.prompts.assistant + '\\n\\n' + '\\n\\n'.join(formated_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reader_scores, reader_cache = evaluate_reader(benchmarks_df, READER, reader_metrics, contexts, \n",
    "                                              show_step=1, cache_relevant_flags=cache_relevant_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_reader_trial_log(SAVE_LOGDIR, reader_scores, SAVE_READER_HYPERPARAMS, SAVE_READERCACHE, \n",
    "                      reader_cache, BENCHMARKS_INFO, BENCHMARKS_MAXSIZE, READER_PARAMS, ADDITIONAL_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage Final. Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL = 4\n",
    "SAVE_LOGDIR = f'./logs/final/trial{TRIAL}'\n",
    "SAVE_READER_HYPERPARAMS = f'{SAVE_LOGDIR}/reader_hyperparams.json'\n",
    "SAVE_READERCACHE = f'{SAVE_LOGDIR}/reader_cache.json'\n",
    "BENCHMARKS_MAXSIZE = 4000\n",
    "\n",
    "ADDITIONAL_PARAMS = {\n",
    "    'bertscore_model_path': \"ru_electra_medium\",\n",
    "    'load_generated_answers': \"./logs/stage2/trial221/reader_cache.json\",\n",
    "    'load_retrievercache': \"./logs/stage1/trial2/retriever_cache.json\"\n",
    "}\n",
    "\n",
    "BENCHMARKS_INFO = {'mtssquad': {'db': 'v3', 'table': 'v3'}}\n",
    "\n",
    "\n",
    "RETRIEVER_PARAMS = {\n",
    "    \"model_path\": f\"/home/dzigen/Desktop/PersonalAI/Personal-AI/models/intfloat/multilingual-e5-small\",\n",
    "    \"densedb_kwargs\": {'metadata': {\"hnsw:space\": \"ip\"}},\n",
    "    \"model_kwargs\": {'device':'cuda'},\n",
    "    \"encode_kwargs\": {'normalize_embeddings': True, 'prompt': 'query: '},\n",
    "    \"params\": {'fetch_k': 10, 'threshold': -1, 'max_k': 10}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, benchmarks_path = prepare_thresholdretriever_configs(BASE_DIR, BENCHMARKS_INFO, RETRIEVER_PARAMS)\n",
    "benchmarks_df = load_benchmarks_df(benchmarks_path, BENCHMARKS_MAXSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Meteor...\n",
      "Loading ExactMatch\n"
     ]
    }
   ],
   "source": [
    "reader_metrics = ReaderMetrics(BASE_DIR, ADDITIONAL_PARAMS['bertscore_model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ADDITIONAL_PARAMS['load_retrievercache'], 'r', encoding='utf-8') as fd:\n",
    "    retrievercache = json.loads(fd.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ADDITIONAL_PARAMS['load_generated_answers'], 'r', encoding='utf-8') as fd:\n",
    "    generated_answers = json.loads(fd.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_relevant_flags = {}\n",
    "for name in benchmarks_df.keys():\n",
    "    cache_relevant_flags[name] = []\n",
    "    for i in range(benchmarks_df[name].shape[0]):\n",
    "        cur_retrieved_ids = list(map(lambda item: item[2]['chunk_id'], retrievercache[name][i]))\n",
    "        target_ids = benchmarks_df[name]['chunk_ids'][i]\n",
    "        cache_relevant_flags[name].append(len(set(target_ids).intersection(set(cur_retrieved_ids))) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3175"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cache_relevant_flags['mtssquad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]/home/dzigen/miniconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/dzigen/miniconda3/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 4000/4000 [06:12<00:00, 10.73it/s, BLEU2=0.266, BLEU1=0.332, ExactMatch=0.0272, METEOR=0.491, ROUGEL=0.3, BertScore=nan, Levenshtain=143, StubScore=1]  \n",
      "/home/dzigen/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "cache = {}\n",
    "show_step = 20\n",
    "for _, name in enumerate(benchmarks_df.keys()):\n",
    "    \n",
    "    scores[name] = {\n",
    "        'BLEU2': [], 'BLEU1': [],\n",
    "        'ExactMatch': [],'METEOR': [],\n",
    "        'ROUGEL': [],\n",
    "        'BertScore': [],\n",
    "        'Levenshtain': [],\n",
    "        'StubScore': [] # отношение числа успешно сгенерированных заглушек к их ожидаемому числу\n",
    "        }\n",
    "    cache[name] = []\n",
    "    \n",
    "    process = tqdm(generated_answers[name])\n",
    "\n",
    "    tmp_target_answers = []\n",
    "    for i, predicted_answer in enumerate(process):\n",
    "        #print(\"answer raw len: \", i, len(predicted_answer))\n",
    "        \n",
    "        target_answer = benchmarks_df[name]['answer'][i]\n",
    "\n",
    "        if cache_relevant_flags is None or cache_relevant_flags[name][i]:\n",
    "            scores[name]['BLEU1'] += reader_metrics.bleu1([predicted_answer], [target_answer])\n",
    "            scores[name]['BLEU2'] += reader_metrics.bleu2([predicted_answer], [target_answer])\n",
    "            scores[name]['ExactMatch'] += reader_metrics.exact_match([predicted_answer], [target_answer])\n",
    "            scores[name]['ROUGEL'] += reader_metrics.rougel([predicted_answer], [target_answer])\n",
    "            scores[name]['METEOR'] += reader_metrics.meteor([predicted_answer], [target_answer])\n",
    "            scores[name]['Levenshtain'] += reader_metrics.levenshtain_score([predicted_answer], [target_answer])\n",
    "        else:\n",
    "            scores[name]['StubScore'].append(1)\n",
    "        \n",
    "        if i % show_step == 0:\n",
    "            process.set_postfix({m_name: np.mean(score) for m_name, score in scores[name].items()})\n",
    "\n",
    "    scores[name] = {m_name: round(float(np.mean(score)), 5) for m_name, score in scores[name].items()}\n",
    "    scores[name]['BertScore'] = reader_metrics.bertscore(generated_answers[name], benchmarks_df[name]['answer'].tolist())\n",
    "    scores[name]['elapsed_time_sec'] = round(float(process.format_dict[\"elapsed\"]), 3)\n",
    "    process.set_postfix(scores[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mtssquad': {'BLEU2': 0.26557,\n",
       "  'BLEU1': 0.33235,\n",
       "  'ExactMatch': 0.02709,\n",
       "  'METEOR': 0.49121,\n",
       "  'ROUGEL': 0.29971,\n",
       "  'BertScore': {'precision': nan,\n",
       "   'recall': 0.34993,\n",
       "   'f1': 0.34445,\n",
       "   'hash': '/home/dzigen/Desktop/Projects/rag_project/models/ru_electra_medium_LNone_no-idf'},\n",
       "  'Levenshtain': 142.98268,\n",
       "  'StubScore': 1.0,\n",
       "  'elapsed_time_sec': 488.141}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_reader_trial_log(SAVE_LOGDIR, scores, SAVE_READER_HYPERPARAMS, SAVE_READERCACHE, \n",
    "                      {}, BENCHMARKS_INFO, BENCHMARKS_MAXSIZE,{}, ADDITIONAL_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
